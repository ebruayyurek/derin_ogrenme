{
  
    
        "post0": {
            "title": "Olasılık",
            "content": "Makine Öğrenmesi tamamen tahminler üretme işidir. Örneğin klinik geçmişi göz önüne alındığında, bir hastanın gelecek yıl kalp krizi geçirme olasılığını tahmin etmek isteyebiliriz. Anormallik tespitinde, bir uçağın jet motorundan gelen bir dizi okumanın normal çalışma için ne kadar olası olduğunu değerlendirmek isteyebiliriz. Pekiştirmeli öğrenmede, robotun ya da daha genel olarak etmenin bir ortamda akıllıca davranmasını isteriz. Bunun anlamı, mevcut eylemlerin her biri altında yüksek bir ödül alma olasılığını düşünmemiz gerektiğidir. Tavsiye sistemleri inşa ettiğimizde de olasılıkları düşünmemiz gerekir. Örneğin, büyük bir online kitapçı için çalıştığımızı varsayalım. Belirli bir kullanıcının belirli bir kitabı satın alma olasılığını tahmin etmek isteyebiliriz. Tüm bu durumlar için olasılık dilini kullanmalıyız. Bir çok kurs, ana bilim dalları, tezler, kariyerler ve hatta bölümler olasılık bilimine üzerine kurulmuştur. Doğal olarak, bu bölümdeki amacımız tüm olasılık konularını öğretmek değildir. Bunun yerine sıfırdan, size ilk derin öğrenme modellerinizi oluşturmaya başlayabileceğiniz kadar olasılık bilgisi öğretmeyi ve isterseniz konunun kendi kendinize keşfetmeye başlayabileceğiniz kadarlık özünü kavratmak istiyoruz. . Açık açık ifade etmesek veya somut örnekler vermesek de, önceki bölümlerde olaslığı kullandık. Şimdi ilk örneğimizle başlayaşım: kedi ve köpekleri fotoğraflardan ayırt etmek. Bu basit gelebilir ama aslında zor bir problemdir. Sorunun zorluğu ilk planda görüntünün çözünürlüğüne bağlı olabilir. . . Şekilde görüldüğü gibi, $160 times 160$ piksel çözünürlüğe sahip resimlerde kedi ve köpekleri seçmek bizim için kolay olsa da, $40 times 40$ piksel için zor hata $10 times 10$ piksel için imkansızdır. Diğer bir ifadeyle, uzaklaştıkça yani çözünürlük azaldıkça kedi ve köpek resimlerini ayırt etmemiz yazı-tura atmaya dönüşmeye başlar. Olasılık sayesinde belirsizlik düzeyini matematiksel olarak ifade edebiliriz. Eğer resmim kedi resmi olduğundan tam olarak eminsek, resme karşılık gelen $y$ etiketinin &quot;kedi&quot; olma olasılığı yani $P(y=$ &quot;kedi&quot;$)$ ifadesi $1$ sayısına eşittir deriz. Fakat $y =$ &quot;kedi&quot; veya that $y =$ &quot;köpek&quot; diyebilmek için herhangi bir bulgu yok ise, olasılıkların eşit olduğunu söyler ve bunu $P(y=$ &quot;kedi&quot;$) = P(y=$ &quot;köpek&quot;$) = 0.5$ şeklinde gösteririz. Eğer resmin kedi resminden olduğu yüksek ihtimal ise fakat kesin emin değilsek, olasılığı $0.5 &lt; P(y=$ &quot;kedi&quot;$) &lt; 1$ gibi bir değer olarak tahmin ederiz. . Şimdi diğer bir örnek verelim. Hava durumu izleme verilerinden yarın bir İstanbul&#39;da yağmur yağma olaslığını tahmin etmek istiyoruz. Yaz aylarındaysak yağmur yaklaşık 0.5 olasılıkla yağar. . İki örnekte de bir şeyin alacağı değerle ilgileniyoruz. Ayrıca ikisinde de sonuçtan kesin emin değiliz. Fakat iki durum arasında kritik bir farklılık var. İlk durumda resim ya kedidir ya da köpektir ve hangisi olduğunu kesin bilmiyoruz. İkinci durumda sonuç rastgele bir olay gibi görülebilir. Dolayısıyla olasılık, belirsizliğin esnek dilidir ve birçok farklı durumda etkin bir şekilde kullanılabilir. . Temel Olas&#305;l&#305;k Kuram&#305; . Say that we cast a die and want to know what the chance is of seeing a 1 rather than another digit. If the die is fair, all the six outcomes $ {1, ldots, 6 }$ are equally likely to occur, and thus we would see a $1$ in one out of six cases. Formally we state that $1$ occurs with probability $ frac{1}{6}$. . For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we will observe a value in $ {1, ldots, 6 }$. Given these outcomes, we want to investigate the probability of observing each outcome. . One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses. This gives us an estimate of the probability of a given event. The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what is going here, let us try it out. . To start, let us import the necessary packages. . %matplotlib inline from d2l import mxnet as d2l from mxnet import np, npx import random npx.set_np() . Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions sampling. The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events. . In MXNet, we can sample from the multinomial distribution via the aptly named np.random.multinomial function. The function can be called in many ways, but we will focus on the simplest. To draw a single sample, we simply pass in a vector of probabilities. The output of the np.random.multinomial function is another vector of the same length: its value at index $i$ is the number of times the sampling outcome corresponds to $i$. . fair_probs = [1.0 / 6] * 6 np.random.multinomial(1, fair_probs) . array([0, 0, 0, 1, 0, 0], dtype=int64) . If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same distribution. It would be unbearably slow to do this with a Python for loop, so random.multinomial supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire. . np.random.multinomial(10, fair_probs) . array([1, 1, 5, 1, 1, 1], dtype=int64) . We can also conduct, say, 3 groups of experiments, where each group draws 10 samples, all at once. . counts = np.random.multinomial(10, fair_probs, size=3) counts . array([[1, 2, 1, 2, 4, 0], [3, 2, 2, 1, 0, 2], [1, 2, 1, 3, 1, 2]], dtype=int64) . Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we calculate the relative frequency as the estimate of the true probability. . # Store the results as 32-bit floats for division counts = np.random.multinomial(1000, fair_probs).astype(np.float32) counts / 1000 # Relative frequency as the estimate . array([0.164, 0.153, 0.181, 0.163, 0.163, 0.176]) . Because we generated the data from a fair die, we know that each outcome has true probability $ frac{1}{6}$, roughly $0.167$, so the above output estimates look good. . We can also visualize how these probabilities converge over time towards the true probability. Let us conduct 500 groups of experiments where each group draws 10 samples. . counts = np.random.multinomial(10, fair_probs, size=500) cum_counts = counts.astype(np.float32).cumsum(axis=0) estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True) d2l.set_figsize((6, 4.5)) for i in range(6): d2l.plt.plot(estimates[:, i].asnumpy(), label=(&quot;P(die=&quot; + str(i + 1) + &quot;)&quot;)) d2l.plt.axhline(y=0.167, color=&#39;black&#39;, linestyle=&#39;dashed&#39;) d2l.plt.gca().set_xlabel(&#39;Groups of experiments&#39;) d2l.plt.gca().set_ylabel(&#39;Estimated probability&#39;) d2l.plt.legend(); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the $6$ solid curves converge towards the true probability. . Olas&#305;l&#305;k Kuram&#305;n&#305;n Aksiyomlar&#305; . When dealing with the rolls of a die, we call the set $ mathcal{S} = {1, 2, 3, 4, 5, 6 }$ the sample space or outcome space, where each element is an outcome. An event is a set of outcomes from a given sample space. For instance, &quot;seeing a $5$&quot; ($ {5 }$) and &quot;seeing an odd number&quot; ($ {1, 3, 5 }$) are both valid events of rolling a die. Note that if the outcome of a random experiment is in event $ mathcal{A}$, then event $ mathcal{A}$ has occurred. That is to say, if $3$ dots faced up after rolling a die, since $3 in {1, 3, 5 }$, we can say that the event &quot;seeing an odd number&quot; has occurred. . Formally, probability can be thought of a function that maps a set to a real value. The probability of an event $ mathcal{A}$ in the given sample space $ mathcal{S}$, denoted as $P( mathcal{A})$, satisfies the following properties: . For any event $ mathcal{A}$, its probability is never negative, i.e., $P( mathcal{A}) geq 0$; | Probability of the entire sample space is $1$, i.e., $P( mathcal{S}) = 1$; | For any countable sequence of events $ mathcal{A}_1, mathcal{A}_2, ldots$ that are mutually exclusive ($ mathcal{A}_i cap mathcal{A}_j = emptyset$ for all $i neq j$), the probability that any happens is equal to the sum of their individual probabilities, i.e., $P( bigcup_{i=1}^{ infty} mathcal{A}_i) = sum_{i=1}^{ infty} P( mathcal{A}_i)$. | . These are also the axioms of probability theory, proposed by Kolmogorov in 1933. Thanks to this axiom system, we can avoid any philosophical dispute on randomness; instead, we can reason rigorously with a mathematical language. For instance, by letting event $ mathcal{A}_1$ be the entire sample space and $ mathcal{A}_i = emptyset$ for all $i &gt; 1$, we can prove that $P( emptyset) = 0$, i.e., the probability of an impossible event is $0$. . Random Variables . In our random experiment of casting a die, we introduced the notion of a random variable. A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Consider a random variable $X$ whose value is in the sample space $ mathcal{S} = {1, 2, 3, 4, 5, 6 }$ of rolling a die. We can denote the event &quot;seeing a $5$&quot; as $ {X = 5 }$ or $X = 5$, and its probability as $P( {X = 5 })$ or $P(X = 5)$. By $P(X = a)$, we make a distinction between the random variable $X$ and the values (e.g., $a$) that $X$ can take. However, such pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote $P(X)$ as the distribution over the random variable $X$: the distribution tells us the probability that $X$ takes any value. On the other hand, we can simply write $P(a)$ to denote the probability that a random variable takes the value $a$. Since an event in probability theory is a set of outcomes from the sample space, we can specify a range of values for a random variable to take. For example, $P(1 leq X leq 3)$ denotes the probability of the event $ {1 leq X leq 3 }$, which means $ {X = 1, 2, text{or}, 3 }$. Equivalently, $P(1 leq X leq 3)$ represents the probability that the random variable $X$ can take a value from $ {1, 2, 3 }$. . Note that there is a subtle difference between discrete random variables, like the sides of a die, and continuous ones, like the weight and the height of a person. There is little point in asking whether two people have exactly the same height. If we take precise enough measurements you will find that no two people on the planet have the exact same height. In fact, if we take a fine enough measurement, you will not have the same height when you wake up and when you go to sleep. So there is no purpose in asking about the probability that someone is 1.80139278291028719210196740527486202 meters tall. Given the world population of humans the probability is virtually 0. It makes more sense in this case to ask whether someone&#39;s height falls into a given interval, say between 1.79 and 1.81 meters. In these cases we quantify the likelihood that we see a value as a density. The height of exactly 1.80 meters has no probability, but nonzero density. In the interval between any two different heights we have nonzero probability. In the rest of this section, we consider probability in discrete space. For probability over continuous random variables, you may refer to :numref:sec_random_variables. . Dealing with Multiple Random Variables . Very often, we will want to consider more than one random variable at a time. For instance, we may want to model the relationship between diseases and symptoms. Given a disease and a symptom, say &quot;flu&quot; and &quot;cough&quot;, either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care. . As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a label, identifying objects in the image. We can also think of the label as a random variable. We can even think of all the metadata as random variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest. . Joint Probability . The first is called the joint probability $P(A = a, B=b)$. Given any values $a$ and $b$, the joint probability lets us answer, what is the probability that $A=a$ and $B=b$ simultaneously? Note that for any values $a$ and $b$, $P(A=a, B=b) leq P(A=a)$. This has to be the case, since for $A=a$ and $B=b$ to happen, $A=a$ has to happen and $B=b$ also has to happen (and vice versa). Thus, $A=a$ and $B=b$ cannot be more likely than $A=a$ or $B=b$ individually. . Conditional Probability . This brings us to an interesting ratio: $0 leq frac{P(A=a, B=b)}{P(A=a)} leq 1$. We call this ratio a conditional probability and denote it by $P(B=b mid A=a)$: it is the probability of $B=b$, provided that $A=a$ has occurred. . Bayes&#39; theorem . Using the definition of conditional probabilities, we can derive one of the most useful and celebrated equations in statistics: Bayes&#39; theorem. It goes as follows. By construction, we have the multiplication rule that $P(A, B) = P(B mid A) P(A)$. By symmetry, this also holds for $P(A, B) = P(A mid B) P(B)$. Assume that $P(B) &gt; 0$. Solving for one of the conditional variables we get . $$P(A mid B) = frac{P(B mid A) P(A)}{P(B)}.$$ . Note that here we use the more compact notation where $P(A, B)$ is a joint distribution and $P(A mid B)$ is a conditional distribution. Such distributions can be evaluated for particular values $A = a, B=b$. . Marginalization . Bayes&#39; theorem is very useful if we want to infer one thing from the other, say cause and effect, but we only know the properties in the reverse direction, as we will see later in this section. One important operation that we need, to make this work, is marginalization. It is the operation of determining $P(B)$ from $P(A, B)$. We can see that the probability of $B$ amounts to accounting for all possible choices of $A$ and aggregating the joint probabilities over all of them: . $$P(B) = sum_{A} P(A, B),$$ . which is also known as the sum rule. The probability or distribution as a result of marginalization is called a marginal probability or a marginal distribution. . Independence . Another useful property to check for is dependence vs. independence. Two random variables $A$ and $B$ being independent means that the occurrence of one event of $A$ does not reveal any information about the occurrence of an event of $B$. In this case $P(B mid A) = P(B)$. Statisticians typically express this as $A perp B$. From Bayes&#39; theorem, it follows immediately that also $P(A mid B) = P(A)$. In all the other cases we call $A$ and $B$ dependent. For instance, two successive rolls of a die are independent. In contrast, the position of a light switch and the brightness in the room are not (they are not perfectly deterministic, though, since we could always have a broken light bulb, power failure, or a broken switch). . Since $P(A mid B) = frac{P(A, B)}{P(B)} = P(A)$ is equivalent to $P(A, B) = P(A)P(B)$, two random variables are independent if and only if their joint distribution is the product of their individual distributions. Likewise, two random variables $A$ and $B$ are conditionally independent given another random variable $C$ if and only if $P(A, B mid C) = P(A mid C)P(B mid C)$. This is expressed as $A perp B mid C$. . Application . :label:subsec_probability_hiv_app . Let us put our skills to the test. Assume that a doctor administers an AIDS test to a patient. This test is fairly accurate and it fails only with 1% probability if the patient is healthy but reporting him as diseased. Moreover, it never fails to detect HIV if the patient actually has it. We use $D_1$ to indicate the diagnosis ($1$ if positive and $0$ if negative) and $H$ to denote the HIV status ($1$ if positive and $0$ if negative). :numref:conditional_prob_D1 lists such conditional probabilities. . :Conditional probability of $P(D_1 mid H)$. . Conditional probability $H=1$ $H=0$ . $P(D_1 = 1 mid H)$ | 1 | 0.01 | . $P(D_1 = 0 mid H)$ | 0 | 0.99 | . :label:conditional_prob_D1 . Note that the column sums are all 1 (but the row sums are not), since the conditional probability needs to sum up to 1, just like the probability. Let us work out the probability of the patient having AIDS if the test comes back positive, i.e., $P(H = 1 mid D_1 = 1)$. Obviously this is going to depend on how common the disease is, since it affects the number of false alarms. Assume that the population is quite healthy, e.g., $P(H=1) = 0.0015$. To apply Bayes&#39; theorem, we need to apply marginalization and the multiplication rule to determine . $$ begin{aligned} &amp;P(D_1 = 1) =&amp; P(D_1=1, H=0) + P(D_1=1, H=1) =&amp; P(D_1=1 mid H=0) P(H=0) + P(D_1=1 mid H=1) P(H=1) =&amp; 0.011485. end{aligned} $$Thus, we get . $$ begin{aligned} &amp;P(H = 1 mid D_1 = 1) =&amp; frac{P(D_1=1 mid H=1) P(H=1)}{P(D_1=1)} =&amp; 0.1306 end{aligned}.$$In other words, there is only a 13.06% chance that the patient actually has AIDS, despite using a very accurate test. As we can see, probability can be counterintuitive. . What should a patient do upon receiving such terrifying news? Likely, the patient would ask the physician to administer another test to get clarity. The second test has different characteristics and it is not as good as the first one, as shown in :numref:conditional_prob_D2. . :Conditional probability of $P(D_2 mid H)$. . Conditional probability $H=1$ $H=0$ . $P(D_2 = 1 mid H)$ | 0.98 | 0.03 | . $P(D_2 = 0 mid H)$ | 0.02 | 0.97 | . :label:conditional_prob_D2 . Unfortunately, the second test comes back positive, too. Let us work out the requisite probabilities to invoke Bayes&#39; theorem by assuming the conditional independence: . $$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1 mid H = 0) =&amp; P(D_1 = 1 mid H = 0) P(D_2 = 1 mid H = 0) =&amp; 0.0003, end{aligned} $$$$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1 mid H = 1) =&amp; P(D_1 = 1 mid H = 1) P(D_2 = 1 mid H = 1) =&amp; 0.98. end{aligned} $$Now we can apply marginalization and the multiplication rule: . $$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1) =&amp; P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1) =&amp; P(D_1 = 1, D_2 = 1 mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 mid H = 1)P(H=1) =&amp; 0.00176955. end{aligned} $$In the end, the probability of the patient having AIDS given both positive tests is . $$ begin{aligned} &amp;P(H = 1 mid D_1 = 1, D_2 = 1) =&amp; frac{P(D_1 = 1, D_2 = 1 mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} =&amp; 0.8307. end{aligned} $$That is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one, it still significantly improved our estimate. . Expectation and Variance . To summarize key characteristics of probability distributions, we need some measures. The expectation (or average) of the random variable $X$ is denoted as . $$E[X] = sum_{x} x P(X = x).$$ . When the input of a function $f(x)$ is a random variable drawn from the distribution $P$ with different values $x$, the expectation of $f(x)$ is computed as . $$E_{x sim P}[f(x)] = sum_x f(x) P(x).$$ . In many cases we want to measure by how much the random variable $X$ deviates from its expectation. This can be quantified by the variance . $$ mathrm{Var}[X] = E left[(X - E[X])^2 right] = E[X^2] - E[X]^2.$$Its square root is called the standard deviation. The variance of a function of a random variable measures by how much the function deviates from the expectation of the function, as different values $x$ of the random variable are sampled from its distribution: . $$ mathrm{Var}[f(x)] = E left[ left(f(x) - E[f(x)] right)^2 right].$$ . Summary . We can sample from probability distributions. | We can analyze multiple random variables using joint distribution, conditional distribution, Bayes&#39; theorem, marginalization, and independence assumptions. | Expectation and variance offer useful measures to summarize key characteristics of probability distributions. | . Exercises . We conducted $m=500$ groups of experiments where each group draws $n=10$ samples. Vary $m$ and $n$. Observe and analyze the experimental results. | Given two events with probability $P( mathcal{A})$ and $P( mathcal{B})$, compute upper and lower bounds on $P( mathcal{A} cup mathcal{B})$ and $P( mathcal{A} cap mathcal{B})$. (Hint: display the situation using a Venn Diagram.) | Assume that we have a sequence of random variables, say $A$, $B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? (Hint: this is a Markov Chain.) | In :numref:subsec_probability_hiv_app, the first test is more accurate. Why not just run the first test a second time? | Discussions .",
            "url": "https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/22/olasilik.html",
            "relUrl": "/jupyter/2020/02/22/olasilik.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "PyTorch",
            "content": "De&#287;i&#351;kenler . Variables are containers for holding data and they&#39;re defined by a name and value. . # Integer variable x = 5 print (x) print (type(x)) . 5 &lt;class &#39;int&#39;&gt; . We can change the value of a variable by simply assigning a new value to it. . # String variable x = &quot;hello&quot; print (x) print (type(x)) . hello &lt;class &#39;str&#39;&gt; . There are many different types of variables: integers, floats, strings, boolean etc. . # int variable x = 5 print (x, type(x)) . 5 &lt;class &#39;int&#39;&gt; . # float variable x = 5.0 print (x, type(x)) . 5.0 &lt;class &#39;float&#39;&gt; . # text variable x = &quot;5&quot; print (x, type(x)) . 5 &lt;class &#39;str&#39;&gt; . # boolean variable x = True print (x, type(x)) . True &lt;class &#39;bool&#39;&gt; . We can also do operations with variables. . # Variables can be used with each other a = 1 b = 2 c = a + b print (c) . 3 . We should always know what types of variables we&#39;re dealing with so we can do the right operations with them. Here&#39;s a common mistake that can happen if we&#39;re using the wrong variable type. . # int variables a = 5 b = 3 print (a + b) . 8 . # string variables a = &quot;5&quot; b = &quot;3&quot; print (a + b) . 53 . Lists . Lists are an ordered, mutable (changeable) collection of values that are comma separated and enclosed by square brackets. A list can be comprised of many different types of variables (below is a list with an integer, string and a float). . # Creating a list x = [3, &quot;hello&quot;, 1.2] print (x) . [3, &#39;hello&#39;, 1.2] . # Length of a list len(x) . 3 . You can add to a list by using the append function. . # Adding to a list x.append(7) print (x) print (len(x)) . [3, &#39;hello&#39;, 1.2, 7] 4 . # Replacing items in a list x[1] = &quot;bye&quot; print (x) . [3, &#39;bye&#39;, 1.2, 7] . # Operations y = [2.4, &quot;world&quot;] z = x + y print (z) . [3, &#39;bye&#39;, 1.2, 7, 2.4, &#39;world&#39;] . Indexing and Slicing . Indexing and slicing from lists allow us to retrieve specific values within lists. Note that indices can be positive (starting from 0) or negative (-1 and lower, where -1 is the last item in the list). . # Indexing x = [3, &quot;hello&quot;, 1.2] print (&quot;x[0]: &quot;, x[0]) print (&quot;x[1]: &quot;, x[1]) print (&quot;x[-1]: &quot;, x[-1]) # the last item print (&quot;x[-2]: &quot;, x[-2]) # the second to last item . x[0]: 3 x[1]: hello x[-1]: 1.2 x[-2]: hello . # Slicing print (&quot;x[:]: &quot;, x[:]) # all indices print (&quot;x[1:]: &quot;, x[1:]) # index 1 to the end of the list print (&quot;x[1:2]: &quot;, x[1:2]) # index 1 to index 2 (not including index 2) print (&quot;x[:-1]: &quot;, x[:-1]) # index 0 to last index (not including last index) . x[:]: [3, &#39;hello&#39;, 1.2] x[1:]: [&#39;hello&#39;, 1.2] x[1:2]: [&#39;hello&#39;] x[:-1]: [3, &#39;hello&#39;] . Tuples . Tuples are collections that are ordered and immutable (unchangeable). You will use these to store values that will never be changed. . # Creating a tuple x = (3.0, &quot;hello&quot;) # tuples start and end with () print (x) . (3.0, &#39;hello&#39;) . # Adding values to a tuple x = x + (5.6, 4) print (x) . (3.0, &#39;hello&#39;, 5.6, 4) . # Try to change (it won&#39;t work and you&#39;ll get an error) x[0] = 1.2 . TypeError Traceback (most recent call last) &lt;ipython-input-19-d0da6f639f74&gt; in &lt;module&gt;() -&gt; 1 x[0] = 1.2 TypeError: &#39;tuple&#39; object does not support item assignment . Dictionaries . Dictionaries are an unordered, mutable and indexed collection of key-value pairs. You can retrieve values based on the key and a dictionary cannot have two of the same keys. . # Creating a dictionary person = {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;brown&#39;} print (person) print (person[&#39;name&#39;]) print (person[&#39;eye_color&#39;]) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;brown&#39;} Goku brown . # Changing the value for a key person[&#39;eye_color&#39;] = &#39;green&#39; print (person) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;green&#39;} . # Adding new key-value pairs person[&#39;age&#39;] = 24 print (person) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;green&#39;, &#39;age&#39;: 24} . # Length of a dictionary print (len(person)) . 3 . If statements . We can use if statements to conditionally do something. The conditions are defined by the words if, elif (which stands for else if) and else. We can have as many elif statements as we want. The indented code below each condition is the code that will execute if the condition is True. . # If statement x = 4 if x &lt; 1: score = &#39;low&#39; elif x &lt;= 4: # elif = else if score = &#39;medium&#39; else: score = &#39;high&#39; print (score) . medium . # If statement with a boolean x = True if x: print (&quot;it worked&quot;) . it worked . Loops . For Loops . A for loop can iterate over a collection of values (lists, tuples, dictionaries, etc.) The indented code is executed for each item in the collection of values. . # For loop veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: print (veggie) . carrots broccoli beans . When the loop encounters the break command, the loop will terminate immediately. If there were more items in the list, they will not be processed. . # `break` from a for loop veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: if veggie == &quot;broccoli&quot;: break print (veggie) . carrots . When the loop encounters the continue command, the loop will skip all other operations for that item in the list only. If there were more items in the list, the loop will continue normally. . # `continue` to the next iteration veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: if veggie == &quot;broccoli&quot;: continue print (veggie) . carrots beans . While Loops . A while loop can perform repeatedly as long as a condition is True. We can use continue and break commands in while loops as well. . # While loop x = 3 while x &gt; 0: x -= 1 # same as x = x - 1 print (x) . 2 1 0 . Functions . Functions are a way to modularize reusable pieces of code. They&#39;re defined by the keyword def which stands for definition and they can have the following components. . # Define the function def add_two(x): &quot;&quot;&quot;Increase x by 2.&quot;&quot;&quot; # explains what this function will do x += 2 return x . Here are the components that may be required when we want to use the function. we need to ensure that the function name and the input parameters match with how we defined the function above. . # Use the function score = 0 new_score = add_two(x=score) print (new_score) . 2 . A function can have as many input parameters and outputs as we want. . # Function with multiple inputs def join_name(first_name, last_name): &quot;&quot;&quot;Combine first name and last name.&quot;&quot;&quot; joined_name = first_name + &quot; &quot; + last_name return joined_name . # Use the function first_name = &quot;Goku&quot; last_name = &quot;Mohandas&quot; joined_name = join_name(first_name=first_name, last_name=last_name) print (joined_name) . Goku Mohandas . NOTE: It&#39;s good practice to always use keyword argument when using a function so that it&#39;s very clear what input variable belongs to what function input parameter. On a related note, you will often see the terms *args and **kwargs which stand for arguments and keyword arguments. You can extract them when they are passed into a function. The significance of the * is that any number of arguments and keyword arguments can be passed into the function. . def f(*args, **kwargs): x = args[0] y = kwargs.get(&#39;y&#39;) print (f&quot;x: {x}, y: {y}&quot;) . f(5, y=2) . x: 5, y: 2 . Classes . Classes are object constructors and are a fundamental component of object oriented programming in Python. They are composed of a set of functions that define the class and it&#39;s operations. . __init__() function . The init function is used when an instance of the class is initialized. . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . &lt;__main__.Pet object at 0x7fe487e9c358&gt; Scooby . __str()__ function . The print (my_dog) command printed something not so relevant to us. Let&#39;s fix that with the __str()__ function. . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name def __str__(self): &quot;&quot;&quot;Output when printing an instance of a Pet.&quot;&quot;&quot; return f&quot;{self.species} named {self.name}&quot; . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . dog named Scooby Scooby . NOTE: Classes can be customized with magic functions like, __str__, to enable powerful operations. We&#39;ll be exploring additional built-in functions in subsequent notebooks (like __iter__ and __getitem__) but if you&#39;re curious, here is a tutorial on more magic methods. . Object methods . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name def __str__(self): &quot;&quot;&quot;Output when printing an instance of a Pet.&quot;&quot;&quot; return f&quot;{self.species} named {self.name}&quot; def change_name(self, new_name): &quot;&quot;&quot;Change the name of your Pet.&quot;&quot;&quot; self.name = new_name . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . dog named Scooby Scooby . # Using a class&#39;s function my_dog.change_name(new_name=&quot;Scrappy&quot;) print (my_dog) print (my_dog.name) . dog named Scrappy Scrappy . Inheritance . Inheritance allows you to inherit all the properties and methods from another class (the parent). Notice how we inherited the initialized variables from the parent Pet class like species and name. We also inherited the change_name function. But for the __str__ function, we define our own version to overwrite the Pet class&#39; __str__ function. . class Dog(Pet): def __init__(self, species, name, breed): super().__init__(&quot;dog&quot;, name) self.breed = breed def __str__(self): return f&quot;{self.breed} named {self.name}&quot; . scooby = Dog(species=&quot;dog&quot;, breed=&quot;Great Dane&quot;, name=&quot;Scooby&quot;) print (scooby) . Great Dane named Scooby . scooby.change_name(&#39;Scooby Doo&#39;) print (scooby) . Great Dane named Scooby Doo . Decorators . Recall that functions allow us to modularize code and reuse them. However, we&#39;ll often want to add some functionality before or after the main function executes and we may want to do this for many different functions. Instead of adding more code to the original function, we can use decorators! . decorators: augment a function with pre/post-processing. Decorators wrap around the main function and allow us to operate on the inputs and or outputs. | . Suppose we have a function called operations which increments the input value x by 1. . def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations(x=1) . 2 . Now let&#39;s say we want to increment our input x by 1 before and after the operations function executes and, to illustrate this example, let&#39;s say the increments have to be separate steps. Here&#39;s how we would do it by changing the original code: . def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 x += 1 x += 1 return x . operations(x=1) . 4 . We were able to achieve what we want but we now increased the size of our operations function and if we want to do the same incrementation for any other function, we have to add the same code to all of those as well ... not very efficient. To solve this, let&#39;s create a decorator called add which increments x by 1 before and after the main function f executes. . Creating a decorator function . The decorator function accepts a function f which is the function we wish to wrap around (in our case, it&#39;s operations). The output of the decorator is its wrapper function which receives the arguments and keyword arguments passed to function f. . Inside the wrapper function, we can extract the input parameters [line 5] passed to function f and make any changes we want [line 6]. Then the function f is executed [line 7] and then we can make changes to the outputs as well [line 8]. Finally, the wrapper function will return some value(s) [line 9] which is what the decorator returns as well since it returns wrapper. . # Decorator def add(f): def wrapper(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) # .get() if not altering x x += 1 # executes before function f x = f(*args, **kwargs, x=x) x += 1 # executes after function f return x return wrapper . We can use this decorator by simply adding it to the top of our main function preceded by the @ symbol. . @add def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations(x=1) . 4 . Suppose we wanted to debug and see what function actually executed with operations. . operations.__name__, operations.__doc__ . (&#39;wrapper&#39;, &#39;Wrapper function for @add.&#39;) . The function name and docstring are not what we&#39;re looking for but it appears this way because the wrapper function is what was executed. In order to fix this, Python offers functools.wraps which carries the main function&#39;s metadata. . from functools import wraps . # Decorator def add(f): @wraps(f) def wrap(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) x += 1 x = f(*args, **kwargs, x=x) x += 1 return x return wrap . @add def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations.__name__, operations.__doc__ . (&#39;operations&#39;, &#39;Basic operations.&#39;) . Awesome! We were able to decorate our main function operation to achieve the customization we wanted without actually altering the function. We can reuse our decorator for other functions that may need the same customization! . This was a dummy example to show how decorators work but we&#39;ll be using them heavily during our production ML lessons. A simple scenario would be using decorators to create uniform JSON responses from each API endpoint without including the bulky code in each endpoint. . Callbacks . Decorators allow for customized operations before and after the main function&#39;s execution but what about in between? Suppose we want to conditionally/situationally do some operations. Instead of writing a whole bunch of if-statements and make our functions bulky, we can use callbacks! . callbacks: conditional/situational processing within the function. | . Our callbacks will be classes that have functions with key names that will execute at various periods during the main function&#39;s execution. The function names are up to us but we need to invoke the same callback functions within our main function. . # Callback class x_tracker(object): def __init__(self, x): self.history = [] def at_start(self, x): self.history.append(x) def at_end(self, x): self.history.append(x) . We can pass in as many callbacks as we want and because they have appropriately named functions, they will be invoked at the appropriate times. . def operations(x, callbacks=[]): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; for callback in callbacks: callback.at_start(x) x += 1 for callback in callbacks: callback.at_end(x) return x . x = 1 tracker = x_tracker(x=x) operations(x=x, callbacks=[tracker]) . 2 . tracker.history . [1, 2] . Putting it all together . decorators + callbacks = powerful customization before, during and after the main function’s execution without increasing its complexity. We will be using this duo to create powerful ML training scripts that are highly customizable in future lessons. . from functools import wraps . # Decorator def add(f): @wraps(f) def wrap(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) # .get() if not altering x x += 1 # executes before function f x = f(*args, **kwargs, x=x) # can do things post function f as well return x return wrap . # Callback class x_tracker(object): def __init__(self, x): self.history = [x] def at_start(self, x): self.history.append(x) def at_end(self, x): self.history.append(x) . # Main function @add def operations(x, callbacks=[]): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; for callback in callbacks: callback.at_start(x) x += 1 for callback in callbacks: callback.at_end(x) return x . x = 1 tracker = x_tracker(x=x) operations(x=x, callbacks=[tracker]) . 3 . tracker.history . [1, 2, 3] . Additional resources . Python 3: This was a very quick look at Python but it&#39;s good enough for practical machine learning and we&#39;ll be learning more in future lessons. If you want to learn more, check out this free Python3 course. | . . Share and discover ML projects at Made With ML. . &nbsp; &nbsp;",
            "url": "https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/05/PyTorch.html",
            "relUrl": "/jupyter/2020/02/05/PyTorch.html",
            "date": " • Feb 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Python",
            "content": "De&#287;i&#351;kenler . Variables are containers for holding data and they&#39;re defined by a name and value. . # Integer variable x = 5 print (x) print (type(x)) . 5 &lt;class &#39;int&#39;&gt; . We can change the value of a variable by simply assigning a new value to it. . # String variable x = &quot;hello&quot; print (x) print (type(x)) . hello &lt;class &#39;str&#39;&gt; . There are many different types of variables: integers, floats, strings, boolean etc. . # int variable x = 5 print (x, type(x)) . 5 &lt;class &#39;int&#39;&gt; . # float variable x = 5.0 print (x, type(x)) . 5.0 &lt;class &#39;float&#39;&gt; . # text variable x = &quot;5&quot; print (x, type(x)) . 5 &lt;class &#39;str&#39;&gt; . # boolean variable x = True print (x, type(x)) . True &lt;class &#39;bool&#39;&gt; . We can also do operations with variables. . # Variables can be used with each other a = 1 b = 2 c = a + b print (c) . 3 . We should always know what types of variables we&#39;re dealing with so we can do the right operations with them. Here&#39;s a common mistake that can happen if we&#39;re using the wrong variable type. . # int variables a = 5 b = 3 print (a + b) . 8 . # string variables a = &quot;5&quot; b = &quot;3&quot; print (a + b) . 53 . Lists . Lists are an ordered, mutable (changeable) collection of values that are comma separated and enclosed by square brackets. A list can be comprised of many different types of variables (below is a list with an integer, string and a float). . # Creating a list x = [3, &quot;hello&quot;, 1.2] print (x) . [3, &#39;hello&#39;, 1.2] . # Length of a list len(x) . 3 . You can add to a list by using the append function. . # Adding to a list x.append(7) print (x) print (len(x)) . [3, &#39;hello&#39;, 1.2, 7] 4 . # Replacing items in a list x[1] = &quot;bye&quot; print (x) . [3, &#39;bye&#39;, 1.2, 7] . # Operations y = [2.4, &quot;world&quot;] z = x + y print (z) . [3, &#39;bye&#39;, 1.2, 7, 2.4, &#39;world&#39;] . Indexing and Slicing . Indexing and slicing from lists allow us to retrieve specific values within lists. Note that indices can be positive (starting from 0) or negative (-1 and lower, where -1 is the last item in the list). . # Indexing x = [3, &quot;hello&quot;, 1.2] print (&quot;x[0]: &quot;, x[0]) print (&quot;x[1]: &quot;, x[1]) print (&quot;x[-1]: &quot;, x[-1]) # the last item print (&quot;x[-2]: &quot;, x[-2]) # the second to last item . x[0]: 3 x[1]: hello x[-1]: 1.2 x[-2]: hello . # Slicing print (&quot;x[:]: &quot;, x[:]) # all indices print (&quot;x[1:]: &quot;, x[1:]) # index 1 to the end of the list print (&quot;x[1:2]: &quot;, x[1:2]) # index 1 to index 2 (not including index 2) print (&quot;x[:-1]: &quot;, x[:-1]) # index 0 to last index (not including last index) . x[:]: [3, &#39;hello&#39;, 1.2] x[1:]: [&#39;hello&#39;, 1.2] x[1:2]: [&#39;hello&#39;] x[:-1]: [3, &#39;hello&#39;] . Tuples . Tuples are collections that are ordered and immutable (unchangeable). You will use these to store values that will never be changed. . # Creating a tuple x = (3.0, &quot;hello&quot;) # tuples start and end with () print (x) . (3.0, &#39;hello&#39;) . # Adding values to a tuple x = x + (5.6, 4) print (x) . (3.0, &#39;hello&#39;, 5.6, 4) . # Try to change (it won&#39;t work and you&#39;ll get an error) x[0] = 1.2 . TypeError Traceback (most recent call last) &lt;ipython-input-19-d0da6f639f74&gt; in &lt;module&gt;() -&gt; 1 x[0] = 1.2 TypeError: &#39;tuple&#39; object does not support item assignment . Dictionaries . Dictionaries are an unordered, mutable and indexed collection of key-value pairs. You can retrieve values based on the key and a dictionary cannot have two of the same keys. . # Creating a dictionary person = {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;brown&#39;} print (person) print (person[&#39;name&#39;]) print (person[&#39;eye_color&#39;]) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;brown&#39;} Goku brown . # Changing the value for a key person[&#39;eye_color&#39;] = &#39;green&#39; print (person) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;green&#39;} . # Adding new key-value pairs person[&#39;age&#39;] = 24 print (person) . {&#39;name&#39;: &#39;Goku&#39;, &#39;eye_color&#39;: &#39;green&#39;, &#39;age&#39;: 24} . # Length of a dictionary print (len(person)) . 3 . If statements . We can use if statements to conditionally do something. The conditions are defined by the words if, elif (which stands for else if) and else. We can have as many elif statements as we want. The indented code below each condition is the code that will execute if the condition is True. . # If statement x = 4 if x &lt; 1: score = &#39;low&#39; elif x &lt;= 4: # elif = else if score = &#39;medium&#39; else: score = &#39;high&#39; print (score) . medium . # If statement with a boolean x = True if x: print (&quot;it worked&quot;) . it worked . Loops . For Loops . A for loop can iterate over a collection of values (lists, tuples, dictionaries, etc.) The indented code is executed for each item in the collection of values. . # For loop veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: print (veggie) . carrots broccoli beans . When the loop encounters the break command, the loop will terminate immediately. If there were more items in the list, they will not be processed. . # `break` from a for loop veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: if veggie == &quot;broccoli&quot;: break print (veggie) . carrots . When the loop encounters the continue command, the loop will skip all other operations for that item in the list only. If there were more items in the list, the loop will continue normally. . # `continue` to the next iteration veggies = [&quot;carrots&quot;, &quot;broccoli&quot;, &quot;beans&quot;] for veggie in veggies: if veggie == &quot;broccoli&quot;: continue print (veggie) . carrots beans . While Loops . A while loop can perform repeatedly as long as a condition is True. We can use continue and break commands in while loops as well. . # While loop x = 3 while x &gt; 0: x -= 1 # same as x = x - 1 print (x) . 2 1 0 . Functions . Functions are a way to modularize reusable pieces of code. They&#39;re defined by the keyword def which stands for definition and they can have the following components. . # Define the function def add_two(x): &quot;&quot;&quot;Increase x by 2.&quot;&quot;&quot; # explains what this function will do x += 2 return x . Here are the components that may be required when we want to use the function. we need to ensure that the function name and the input parameters match with how we defined the function above. . # Use the function score = 0 new_score = add_two(x=score) print (new_score) . 2 . A function can have as many input parameters and outputs as we want. . # Function with multiple inputs def join_name(first_name, last_name): &quot;&quot;&quot;Combine first name and last name.&quot;&quot;&quot; joined_name = first_name + &quot; &quot; + last_name return joined_name . # Use the function first_name = &quot;Goku&quot; last_name = &quot;Mohandas&quot; joined_name = join_name(first_name=first_name, last_name=last_name) print (joined_name) . Goku Mohandas . NOTE: It&#39;s good practice to always use keyword argument when using a function so that it&#39;s very clear what input variable belongs to what function input parameter. On a related note, you will often see the terms *args and **kwargs which stand for arguments and keyword arguments. You can extract them when they are passed into a function. The significance of the * is that any number of arguments and keyword arguments can be passed into the function. . def f(*args, **kwargs): x = args[0] y = kwargs.get(&#39;y&#39;) print (f&quot;x: {x}, y: {y}&quot;) . f(5, y=2) . x: 5, y: 2 . Classes . Classes are object constructors and are a fundamental component of object oriented programming in Python. They are composed of a set of functions that define the class and it&#39;s operations. . __init__() function . The init function is used when an instance of the class is initialized. . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . &lt;__main__.Pet object at 0x7fe487e9c358&gt; Scooby . __str()__ function . The print (my_dog) command printed something not so relevant to us. Let&#39;s fix that with the __str()__ function. . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name def __str__(self): &quot;&quot;&quot;Output when printing an instance of a Pet.&quot;&quot;&quot; return f&quot;{self.species} named {self.name}&quot; . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . dog named Scooby Scooby . NOTE: Classes can be customized with magic functions like, __str__, to enable powerful operations. We&#39;ll be exploring additional built-in functions in subsequent notebooks (like __iter__ and __getitem__) but if you&#39;re curious, here is a tutorial on more magic methods. . Object methods . # Creating the class class Pet(object): &quot;&quot;&quot;Class object for a pet.&quot;&quot;&quot; def __init__(self, species, name): &quot;&quot;&quot;Initialize a Pet.&quot;&quot;&quot; self.species = species self.name = name def __str__(self): &quot;&quot;&quot;Output when printing an instance of a Pet.&quot;&quot;&quot; return f&quot;{self.species} named {self.name}&quot; def change_name(self, new_name): &quot;&quot;&quot;Change the name of your Pet.&quot;&quot;&quot; self.name = new_name . # Creating an instance of a class my_dog = Pet(species=&quot;dog&quot;, name=&quot;Scooby&quot;) print (my_dog) print (my_dog.name) . dog named Scooby Scooby . # Using a class&#39;s function my_dog.change_name(new_name=&quot;Scrappy&quot;) print (my_dog) print (my_dog.name) . dog named Scrappy Scrappy . Inheritance . Inheritance allows you to inherit all the properties and methods from another class (the parent). Notice how we inherited the initialized variables from the parent Pet class like species and name. We also inherited the change_name function. But for the __str__ function, we define our own version to overwrite the Pet class&#39; __str__ function. . class Dog(Pet): def __init__(self, species, name, breed): super().__init__(&quot;dog&quot;, name) self.breed = breed def __str__(self): return f&quot;{self.breed} named {self.name}&quot; . scooby = Dog(species=&quot;dog&quot;, breed=&quot;Great Dane&quot;, name=&quot;Scooby&quot;) print (scooby) . Great Dane named Scooby . scooby.change_name(&#39;Scooby Doo&#39;) print (scooby) . Great Dane named Scooby Doo . Decorators . Recall that functions allow us to modularize code and reuse them. However, we&#39;ll often want to add some functionality before or after the main function executes and we may want to do this for many different functions. Instead of adding more code to the original function, we can use decorators! . decorators: augment a function with pre/post-processing. Decorators wrap around the main function and allow us to operate on the inputs and or outputs. | . Suppose we have a function called operations which increments the input value x by 1. . def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations(x=1) . 2 . Now let&#39;s say we want to increment our input x by 1 before and after the operations function executes and, to illustrate this example, let&#39;s say the increments have to be separate steps. Here&#39;s how we would do it by changing the original code: . def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 x += 1 x += 1 return x . operations(x=1) . 4 . We were able to achieve what we want but we now increased the size of our operations function and if we want to do the same incrementation for any other function, we have to add the same code to all of those as well ... not very efficient. To solve this, let&#39;s create a decorator called add which increments x by 1 before and after the main function f executes. . Creating a decorator function . The decorator function accepts a function f which is the function we wish to wrap around (in our case, it&#39;s operations). The output of the decorator is its wrapper function which receives the arguments and keyword arguments passed to function f. . Inside the wrapper function, we can extract the input parameters [line 5] passed to function f and make any changes we want [line 6]. Then the function f is executed [line 7] and then we can make changes to the outputs as well [line 8]. Finally, the wrapper function will return some value(s) [line 9] which is what the decorator returns as well since it returns wrapper. . # Decorator def add(f): def wrapper(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) # .get() if not altering x x += 1 # executes before function f x = f(*args, **kwargs, x=x) x += 1 # executes after function f return x return wrapper . We can use this decorator by simply adding it to the top of our main function preceded by the @ symbol. . @add def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations(x=1) . 4 . Suppose we wanted to debug and see what function actually executed with operations. . operations.__name__, operations.__doc__ . (&#39;wrapper&#39;, &#39;Wrapper function for @add.&#39;) . The function name and docstring are not what we&#39;re looking for but it appears this way because the wrapper function is what was executed. In order to fix this, Python offers functools.wraps which carries the main function&#39;s metadata. . from functools import wraps . # Decorator def add(f): @wraps(f) def wrap(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) x += 1 x = f(*args, **kwargs, x=x) x += 1 return x return wrap . @add def operations(x): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; x += 1 return x . operations.__name__, operations.__doc__ . (&#39;operations&#39;, &#39;Basic operations.&#39;) . Awesome! We were able to decorate our main function operation to achieve the customization we wanted without actually altering the function. We can reuse our decorator for other functions that may need the same customization! . This was a dummy example to show how decorators work but we&#39;ll be using them heavily during our production ML lessons. A simple scenario would be using decorators to create uniform JSON responses from each API endpoint without including the bulky code in each endpoint. . Callbacks . Decorators allow for customized operations before and after the main function&#39;s execution but what about in between? Suppose we want to conditionally/situationally do some operations. Instead of writing a whole bunch of if-statements and make our functions bulky, we can use callbacks! . callbacks: conditional/situational processing within the function. | . Our callbacks will be classes that have functions with key names that will execute at various periods during the main function&#39;s execution. The function names are up to us but we need to invoke the same callback functions within our main function. . # Callback class x_tracker(object): def __init__(self, x): self.history = [] def at_start(self, x): self.history.append(x) def at_end(self, x): self.history.append(x) . We can pass in as many callbacks as we want and because they have appropriately named functions, they will be invoked at the appropriate times. . def operations(x, callbacks=[]): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; for callback in callbacks: callback.at_start(x) x += 1 for callback in callbacks: callback.at_end(x) return x . x = 1 tracker = x_tracker(x=x) operations(x=x, callbacks=[tracker]) . 2 . tracker.history . [1, 2] . Putting it all together . decorators + callbacks = powerful customization before, during and after the main function’s execution without increasing its complexity. We will be using this duo to create powerful ML training scripts that are highly customizable in future lessons. . from functools import wraps . # Decorator def add(f): @wraps(f) def wrap(*args, **kwargs): &quot;&quot;&quot;Wrapper function for @add.&quot;&quot;&quot; x = kwargs.pop(&#39;x&#39;) # .get() if not altering x x += 1 # executes before function f x = f(*args, **kwargs, x=x) # can do things post function f as well return x return wrap . # Callback class x_tracker(object): def __init__(self, x): self.history = [x] def at_start(self, x): self.history.append(x) def at_end(self, x): self.history.append(x) . # Main function @add def operations(x, callbacks=[]): &quot;&quot;&quot;Basic operations.&quot;&quot;&quot; for callback in callbacks: callback.at_start(x) x += 1 for callback in callbacks: callback.at_end(x) return x . x = 1 tracker = x_tracker(x=x) operations(x=x, callbacks=[tracker]) . 3 . tracker.history . [1, 2, 3] . Additional resources . Python 3: This was a very quick look at Python but it&#39;s good enough for practical machine learning and we&#39;ll be learning more in future lessons. If you want to learn more, check out this free Python3 course. | . . Share and discover ML projects at Made With ML. . &nbsp; &nbsp;",
            "url": "https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/01/Python.html",
            "relUrl": "/jupyter/2020/02/01/Python.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Numpy",
            "content": "Set up . import numpy as np . # Set seed for reproducibility np.random.seed(seed=1234) . Basics . Let&#39;s take a took at how to create tensors with NumPy. . Tensor: collection of values | . # Scalar x = np.array(6) # scalar print (&quot;x: &quot;, x) # Number of dimensions print (&quot;x ndim: &quot;, x.ndim) # Dimensions print (&quot;x shape:&quot;, x.shape) # Size of elements print (&quot;x size: &quot;, x.size) # Data type print (&quot;x dtype: &quot;, x.dtype) . x: 6 x ndim: 0 x shape: () x size: 1 x dtype: int64 . # Vector x = np.array([1.3 , 2.2 , 1.7]) print (&quot;x: &quot;, x) print (&quot;x ndim: &quot;, x.ndim) print (&quot;x shape:&quot;, x.shape) print (&quot;x size: &quot;, x.size) print (&quot;x dtype: &quot;, x.dtype) # notice the float datatype . x: [1.3 2.2 1.7] x ndim: 1 x shape: (3,) x size: 3 x dtype: float64 . # Matrix x = np.array([[1,2], [3,4]]) print (&quot;x: n&quot;, x) print (&quot;x ndim: &quot;, x.ndim) print (&quot;x shape:&quot;, x.shape) print (&quot;x size: &quot;, x.size) print (&quot;x dtype: &quot;, x.dtype) . x: [[1 2] [3 4]] x ndim: 2 x shape: (2, 2) x size: 4 x dtype: int64 . # 3-D Tensor x = np.array([[[1,2],[3,4]],[[5,6],[7,8]]]) print (&quot;x: n&quot;, x) print (&quot;x ndim: &quot;, x.ndim) print (&quot;x shape:&quot;, x.shape) print (&quot;x size: &quot;, x.size) print (&quot;x dtype: &quot;, x.dtype) . x: [[[1 2] [3 4]] [[5 6] [7 8]]] x ndim: 3 x shape: (2, 2, 2) x size: 8 x dtype: int64 . NumPy also comes with several functions that allow us to create tensors quickly. . # Functions print (&quot;np.zeros((2,2)): n&quot;, np.zeros((2,2))) print (&quot;np.ones((2,2)): n&quot;, np.ones((2,2))) print (&quot;np.eye((2)): n&quot;, np.eye((2))) # identity matrix print (&quot;np.random.random((2,2)): n&quot;, np.random.random((2,2))) . np.zeros((2,2)): [[0. 0.] [0. 0.]] np.ones((2,2)): [[1. 1.] [1. 1.]] np.eye((2)): [[1. 0.] [0. 1.]] np.random.random((2,2)): [[0.19151945 0.62210877] [0.43772774 0.78535858]] . Indexing . Keep in mind that when indexing the row and column, indices start at 0. And like indexing with lists, we can use negative indices as well (where -1 is the last item). . # Indexing x = np.array([1, 2, 3]) print (&quot;x: &quot;, x) print (&quot;x[0]: &quot;, x[0]) x[0] = 0 print (&quot;x: &quot;, x) . x: [1 2 3] x[0]: 1 x: [0 2 3] . # Slicing x = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) print (x) print (&quot;x column 1: &quot;, x[:, 1]) print (&quot;x row 0: &quot;, x[0, :]) print (&quot;x rows 0,1 &amp; cols 1,2: n&quot;, x[0:2, 1:3]) . [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] x column 1: [ 2 6 10] x row 0: [1 2 3 4] x rows 0,1 &amp; cols 1,2: [[2 3] [6 7]] . # Integer array indexing print (x) rows_to_get = np.array([0, 1, 2]) print (&quot;rows_to_get: &quot;, rows_to_get) cols_to_get = np.array([0, 2, 1]) print (&quot;cols_to_get: &quot;, cols_to_get) # Combine sequences above to get values to get print (&quot;indexed values: &quot;, x[rows_to_get, cols_to_get]) # (0, 0), (1, 2), (2, 1) . [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] rows_to_get: [0 1 2] cols_to_get: [0 2 1] indexed values: [ 1 7 10] . # Boolean array indexing x = np.array([[1, 2], [3, 4], [5, 6]]) print (&quot;x: n&quot;, x) print (&quot;x &gt; 2: n&quot;, x &gt; 2) print (&quot;x[x &gt; 2]: n&quot;, x[x &gt; 2]) . x: [[1 2] [3 4] [5 6]] x &gt; 2: [[False False] [ True True] [ True True]] x[x &gt; 2]: [3 4 5 6] . Arithmetic . # Basic math x = np.array([[1,2], [3,4]], dtype=np.float64) y = np.array([[1,2], [3,4]], dtype=np.float64) print (&quot;x + y: n&quot;, np.add(x, y)) # or x + y print (&quot;x - y: n&quot;, np.subtract(x, y)) # or x - y print (&quot;x * y: n&quot;, np.multiply(x, y)) # or x * y . x + y: [[2. 4.] [6. 8.]] x - y: [[0. 0.] [0. 0.]] x * y: [[ 1. 4.] [ 9. 16.]] . Dot product . One of the most common NumPy operations we’ll use in machine learning is matrix multiplication using the dot product. We take the rows of our first matrix (2) and the columns of our second matrix (2) to determine the dot product, giving us an output of [2 X 2]. The only requirement is that the inside dimensions match, in this case the first matrix has 3 columns and the second matrix has 3 rows. . # Dot product a = np.array([[1,2,3], [4,5,6]], dtype=np.float64) # we can specify dtype b = np.array([[7,8], [9,10], [11, 12]], dtype=np.float64) c = a.dot(b) print (f&quot;{a.shape} · {b.shape} = {c.shape}&quot;) print (c) . (2, 3) · (3, 2) = (2, 2) [[ 58. 64.] [139. 154.]] . Axis operations . We can also do operations across a specific axis. . # Sum across a dimension x = np.array([[1,2],[3,4]]) print (x) print (&quot;sum all: &quot;, np.sum(x)) # adds all elements print (&quot;sum axis=0: &quot;, np.sum(x, axis=0)) # sum across rows print (&quot;sum axis=1: &quot;, np.sum(x, axis=1)) # sum across columns . [[1 2] [3 4]] sum all: 10 sum axis=0: [4 6] sum axis=1: [3 7] . # Min/max x = np.array([[1,2,3], [4,5,6]]) print (&quot;min: &quot;, x.min()) print (&quot;max: &quot;, x.max()) print (&quot;min axis=0: &quot;, x.min(axis=0)) print (&quot;min axis=1: &quot;, x.min(axis=1)) . min: 1 max: 6 min axis=0: [1 2 3] min axis=1: [1 4] . Broadcasting . Here, we’re adding a vector with a scalar. Their dimensions aren’t compatible as is but how does NumPy still gives us the right result? This is where broadcasting comes in. The scalar is broadcast across the vector so that they have compatible shapes. . # Broadcasting x = np.array([1,2]) # vector y = np.array(3) # scalar z = x + y print (&quot;z: n&quot;, z) . z: [4 5] . Advanced . Transposing . We often need to change the dimensions of our tensors for operations like the dot product. If we need to switch two dimensions, we can transpose the tensor. . # Transposing x = np.array([[1,2,3], [4,5,6]]) print (&quot;x: n&quot;, x) print (&quot;x.shape: &quot;, x.shape) y = np.transpose(x, (1,0)) # flip dimensions at index 0 and 1 print (&quot;y: n&quot;, y) print (&quot;y.shape: &quot;, y.shape) . x: [[1 2 3] [4 5 6]] x.shape: (2, 3) y: [[1 4] [2 5] [3 6]] y.shape: (3, 2) . Reshaping . Sometimes, we&#39;ll need to alter the dimensions of the matrix. Reshaping allows us to transform a tensor into different permissible shapes -- our reshaped tensor has the same amount of values in the tensor. (1X6 = 2X3). We can also use -1 on a dimension and NumPy will infer the dimension based on our input tensor. . The way reshape works is by looking at each dimension of the new tensor and separating our original tensor into that many units. So here the dimension at index 0 of the new tensor is 2 so we divide our original tensor into 2 units, and each of those has 3 values. . # Reshaping x = np.array([[1,2,3,4,5,6]]) print (x) print (&quot;x.shape: &quot;, x.shape) y = np.reshape(x, (2, 3)) print (&quot;y: n&quot;, y) print (&quot;y.shape: &quot;, y.shape) z = np.reshape(x, (2, -1)) print (&quot;z: n&quot;, z) print (&quot;z.shape: &quot;, z.shape) . [[1 2 3 4 5 6]] x.shape: (1, 6) y: [[1 2 3] [4 5 6]] y.shape: (2, 3) z: [[1 2 3] [4 5 6]] z.shape: (2, 3) . Unintended reshaping . Though reshaping is very convenient to manipulate tensors, we must be careful of their pitfalls as well. Let&#39;s look at the example below. Suppose we have x, which has the shape [2 X 3 X 4]. . [[[ 1 1 1 1] [ 2 2 2 2] [ 3 3 3 3]] [[10 10 10 10] [20 20 20 20] [30 30 30 30]]] . We want to reshape x so that it has shape [3 X 8] which we&#39;ll get by moving the dimension at index 0 to become the dimension at index 1 and then combining the last two dimensions. But when we do this, we want our output . to look like: ✅ . [[ 1 1 1 1 10 10 10 10] [ 2 2 2 2 20 20 20 20] [ 3 3 3 3 30 30 30 30]] . and not like: ❌ . [[ 1 1 1 1 2 2 2 2] [ 3 3 3 3 10 10 10 10] [20 20 20 20 30 30 30 30]] . even though they both have the same shape [3X8]. . x = np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], [[10, 10, 10, 10], [20, 20, 20, 20], [30, 30, 30, 30]]]) print (&quot;x: n&quot;, x) print (&quot;x.shape: &quot;, x.shape) . x: [[[ 1 1 1 1] [ 2 2 2 2] [ 3 3 3 3]] [[10 10 10 10] [20 20 20 20] [30 30 30 30]]] x.shape: (2, 3, 4) . When we naively do a reshape, we get the right shape but the values are not what we&#39;re looking for. . # Unintended reshaping z_incorrect = np.reshape(x, (x.shape[1], -1)) print (&quot;z_incorrect: n&quot;, z_incorrect) print (&quot;z_incorrect.shape: &quot;, z_incorrect.shape) . z_incorrect: [[ 1 1 1 1 2 2 2 2] [ 3 3 3 3 10 10 10 10] [20 20 20 20 30 30 30 30]] z_incorrect.shape: (3, 8) . Instead, if we transpose the tensor and then do a reshape, we get our desired tensor. Transpose allows us to put our two vectors that we want to combine together and then we use reshape to join them together. Always create a dummy example like this when you’re unsure about reshaping. Blindly going by the tensor shape can lead to lots of issues downstream. . # Intended reshaping y = np.transpose(x, (1,0,2)) print (&quot;y: n&quot;, y) print (&quot;y.shape: &quot;, y.shape) z_correct = np.reshape(y, (y.shape[0], -1)) print (&quot;z_correct: n&quot;, z_correct) print (&quot;z_correct.shape: &quot;, z_correct.shape) . y: [[[ 1 1 1 1] [10 10 10 10]] [[ 2 2 2 2] [20 20 20 20]] [[ 3 3 3 3] [30 30 30 30]]] y.shape: (3, 2, 4) z_correct: [[ 1 1 1 1 10 10 10 10] [ 2 2 2 2 20 20 20 20] [ 3 3 3 3 30 30 30 30]] z_correct.shape: (3, 8) . Adding/removing dimensions . We can also easily add and remove dimensions to our tensors and we&#39;ll want to do this to make tensors compatible for certain operations. . # Adding dimensions x = np.array([[1,2,3],[4,5,6]]) print (&quot;x: n&quot;, x) print (&quot;x.shape: &quot;, x.shape) y = np.expand_dims(x, 1) # expand dim 1 print (&quot;y: n&quot;, y) print (&quot;y.shape: &quot;, y.shape) # notice extra set of brackets are added . x: [[1 2 3] [4 5 6]] x.shape: (2, 3) y: [[[1 2 3]] [[4 5 6]]] y.shape: (2, 1, 3) . # Removing dimensions x = np.array([[[1,2,3]],[[4,5,6]]]) print (&quot;x: n&quot;, x) print (&quot;x.shape: &quot;, x.shape) y = np.squeeze(x, 1) # squeeze dim 1 print (&quot;y: n&quot;, y) print (&quot;y.shape: &quot;, y.shape) # notice extra set of brackets are gone . x: [[[1 2 3]] [[4 5 6]]] x.shape: (2, 1, 3) y: [[1 2 3] [4 5 6]] y.shape: (2, 3) . Additional resources . NumPy reference manual: We don&#39;t have to memorize anything here and we will be taking a closer look at NumPy in the later lessons. If you want to learn more checkout the NumPy reference manual. | . . Share and discover ML projects at Made With ML. . &nbsp; &nbsp;",
            "url": "https://unverciftci.github.io/derin_ogrenme/jupyter/2020/01/01/NumPy.html",
            "relUrl": "/jupyter/2020/01/01/NumPy.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Bilgi",
          "content": "Bu kitap Derin Öğrenme konusunda bazı temel konuları içermektedir. . Hata ve eklemeler için: unverciftci@gmail.com. .",
          "url": "https://unverciftci.github.io/derin_ogrenme/bilgi/",
          "relUrl": "/bilgi/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://unverciftci.github.io/derin_ogrenme/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}