{
  
    
        "post0": {
            "title": "Probability",
            "content": "%matplotlib inline from d2l import mxnet as d2l from mxnet import np, npx import random npx.set_np() . Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions sampling. The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events. . In MXNet, we can sample from the multinomial distribution via the aptly named np.random.multinomial function. The function can be called in many ways, but we will focus on the simplest. To draw a single sample, we simply pass in a vector of probabilities. The output of the np.random.multinomial function is another vector of the same length: its value at index $i$ is the number of times the sampling outcome corresponds to $i$. . fair_probs = [1.0 / 6] * 6 np.random.multinomial(1, fair_probs) . array([0, 0, 0, 1, 0, 0], dtype=int64) . If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same distribution. It would be unbearably slow to do this with a Python for loop, so random.multinomial supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire. . np.random.multinomial(10, fair_probs) . array([1, 1, 5, 1, 1, 1], dtype=int64) . We can also conduct, say, 3 groups of experiments, where each group draws 10 samples, all at once. . counts = np.random.multinomial(10, fair_probs, size=3) counts . array([[1, 2, 1, 2, 4, 0], [3, 2, 2, 1, 0, 2], [1, 2, 1, 3, 1, 2]], dtype=int64) . Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we calculate the relative frequency as the estimate of the true probability. . # Store the results as 32-bit floats for division counts = np.random.multinomial(1000, fair_probs).astype(np.float32) counts / 1000 # Relative frequency as the estimate . array([0.164, 0.153, 0.181, 0.163, 0.163, 0.176]) . Because we generated the data from a fair die, we know that each outcome has true probability $ frac{1}{6}$, roughly $0.167$, so the above output estimates look good. . We can also visualize how these probabilities converge over time towards the true probability. Let us conduct 500 groups of experiments where each group draws 10 samples. . counts = np.random.multinomial(10, fair_probs, size=500) cum_counts = counts.astype(np.float32).cumsum(axis=0) estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True) d2l.set_figsize((6, 4.5)) for i in range(6): d2l.plt.plot(estimates[:, i].asnumpy(), label=(&quot;P(die=&quot; + str(i + 1) + &quot;)&quot;)) d2l.plt.axhline(y=0.167, color=&#39;black&#39;, linestyle=&#39;dashed&#39;) d2l.plt.gca().set_xlabel(&#39;Groups of experiments&#39;) d2l.plt.gca().set_ylabel(&#39;Estimated probability&#39;) d2l.plt.legend(); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the $6$ solid curves converge towards the true probability. . Axioms of Probability Theory . When dealing with the rolls of a die, we call the set $ mathcal{S} = {1, 2, 3, 4, 5, 6 }$ the sample space or outcome space, where each element is an outcome. An event is a set of outcomes from a given sample space. For instance, &quot;seeing a $5$&quot; ($ {5 }$) and &quot;seeing an odd number&quot; ($ {1, 3, 5 }$) are both valid events of rolling a die. Note that if the outcome of a random experiment is in event $ mathcal{A}$, then event $ mathcal{A}$ has occurred. That is to say, if $3$ dots faced up after rolling a die, since $3 in {1, 3, 5 }$, we can say that the event &quot;seeing an odd number&quot; has occurred. . Formally, probability can be thought of a function that maps a set to a real value. The probability of an event $ mathcal{A}$ in the given sample space $ mathcal{S}$, denoted as $P( mathcal{A})$, satisfies the following properties: . For any event $ mathcal{A}$, its probability is never negative, i.e., $P( mathcal{A}) geq 0$; | Probability of the entire sample space is $1$, i.e., $P( mathcal{S}) = 1$; | For any countable sequence of events $ mathcal{A}_1, mathcal{A}_2, ldots$ that are mutually exclusive ($ mathcal{A}_i cap mathcal{A}_j = emptyset$ for all $i neq j$), the probability that any happens is equal to the sum of their individual probabilities, i.e., $P( bigcup_{i=1}^{ infty} mathcal{A}_i) = sum_{i=1}^{ infty} P( mathcal{A}_i)$. | . These are also the axioms of probability theory, proposed by Kolmogorov in 1933. Thanks to this axiom system, we can avoid any philosophical dispute on randomness; instead, we can reason rigorously with a mathematical language. For instance, by letting event $ mathcal{A}_1$ be the entire sample space and $ mathcal{A}_i = emptyset$ for all $i &gt; 1$, we can prove that $P( emptyset) = 0$, i.e., the probability of an impossible event is $0$. . Random Variables . In our random experiment of casting a die, we introduced the notion of a random variable. A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Consider a random variable $X$ whose value is in the sample space $ mathcal{S} = {1, 2, 3, 4, 5, 6 }$ of rolling a die. We can denote the event &quot;seeing a $5$&quot; as $ {X = 5 }$ or $X = 5$, and its probability as $P( {X = 5 })$ or $P(X = 5)$. By $P(X = a)$, we make a distinction between the random variable $X$ and the values (e.g., $a$) that $X$ can take. However, such pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote $P(X)$ as the distribution over the random variable $X$: the distribution tells us the probability that $X$ takes any value. On the other hand, we can simply write $P(a)$ to denote the probability that a random variable takes the value $a$. Since an event in probability theory is a set of outcomes from the sample space, we can specify a range of values for a random variable to take. For example, $P(1 leq X leq 3)$ denotes the probability of the event $ {1 leq X leq 3 }$, which means $ {X = 1, 2, text{or}, 3 }$. Equivalently, $P(1 leq X leq 3)$ represents the probability that the random variable $X$ can take a value from $ {1, 2, 3 }$. . Note that there is a subtle difference between discrete random variables, like the sides of a die, and continuous ones, like the weight and the height of a person. There is little point in asking whether two people have exactly the same height. If we take precise enough measurements you will find that no two people on the planet have the exact same height. In fact, if we take a fine enough measurement, you will not have the same height when you wake up and when you go to sleep. So there is no purpose in asking about the probability that someone is 1.80139278291028719210196740527486202 meters tall. Given the world population of humans the probability is virtually 0. It makes more sense in this case to ask whether someone&#39;s height falls into a given interval, say between 1.79 and 1.81 meters. In these cases we quantify the likelihood that we see a value as a density. The height of exactly 1.80 meters has no probability, but nonzero density. In the interval between any two different heights we have nonzero probability. In the rest of this section, we consider probability in discrete space. For probability over continuous random variables, you may refer to :numref:sec_random_variables. . Dealing with Multiple Random Variables . Very often, we will want to consider more than one random variable at a time. For instance, we may want to model the relationship between diseases and symptoms. Given a disease and a symptom, say &quot;flu&quot; and &quot;cough&quot;, either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care. . As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a label, identifying objects in the image. We can also think of the label as a random variable. We can even think of all the metadata as random variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest. . Joint Probability . The first is called the joint probability $P(A = a, B=b)$. Given any values $a$ and $b$, the joint probability lets us answer, what is the probability that $A=a$ and $B=b$ simultaneously? Note that for any values $a$ and $b$, $P(A=a, B=b) leq P(A=a)$. This has to be the case, since for $A=a$ and $B=b$ to happen, $A=a$ has to happen and $B=b$ also has to happen (and vice versa). Thus, $A=a$ and $B=b$ cannot be more likely than $A=a$ or $B=b$ individually. . Conditional Probability . This brings us to an interesting ratio: $0 leq frac{P(A=a, B=b)}{P(A=a)} leq 1$. We call this ratio a conditional probability and denote it by $P(B=b mid A=a)$: it is the probability of $B=b$, provided that $A=a$ has occurred. . Bayes&#39; theorem . Using the definition of conditional probabilities, we can derive one of the most useful and celebrated equations in statistics: Bayes&#39; theorem. It goes as follows. By construction, we have the multiplication rule that $P(A, B) = P(B mid A) P(A)$. By symmetry, this also holds for $P(A, B) = P(A mid B) P(B)$. Assume that $P(B) &gt; 0$. Solving for one of the conditional variables we get . $$P(A mid B) = frac{P(B mid A) P(A)}{P(B)}.$$ . Note that here we use the more compact notation where $P(A, B)$ is a joint distribution and $P(A mid B)$ is a conditional distribution. Such distributions can be evaluated for particular values $A = a, B=b$. . Marginalization . Bayes&#39; theorem is very useful if we want to infer one thing from the other, say cause and effect, but we only know the properties in the reverse direction, as we will see later in this section. One important operation that we need, to make this work, is marginalization. It is the operation of determining $P(B)$ from $P(A, B)$. We can see that the probability of $B$ amounts to accounting for all possible choices of $A$ and aggregating the joint probabilities over all of them: . $$P(B) = sum_{A} P(A, B),$$ . which is also known as the sum rule. The probability or distribution as a result of marginalization is called a marginal probability or a marginal distribution. . Independence . Another useful property to check for is dependence vs. independence. Two random variables $A$ and $B$ being independent means that the occurrence of one event of $A$ does not reveal any information about the occurrence of an event of $B$. In this case $P(B mid A) = P(B)$. Statisticians typically express this as $A perp B$. From Bayes&#39; theorem, it follows immediately that also $P(A mid B) = P(A)$. In all the other cases we call $A$ and $B$ dependent. For instance, two successive rolls of a die are independent. In contrast, the position of a light switch and the brightness in the room are not (they are not perfectly deterministic, though, since we could always have a broken light bulb, power failure, or a broken switch). . Since $P(A mid B) = frac{P(A, B)}{P(B)} = P(A)$ is equivalent to $P(A, B) = P(A)P(B)$, two random variables are independent if and only if their joint distribution is the product of their individual distributions. Likewise, two random variables $A$ and $B$ are conditionally independent given another random variable $C$ if and only if $P(A, B mid C) = P(A mid C)P(B mid C)$. This is expressed as $A perp B mid C$. . Application . :label:subsec_probability_hiv_app . Let us put our skills to the test. Assume that a doctor administers an AIDS test to a patient. This test is fairly accurate and it fails only with 1% probability if the patient is healthy but reporting him as diseased. Moreover, it never fails to detect HIV if the patient actually has it. We use $D_1$ to indicate the diagnosis ($1$ if positive and $0$ if negative) and $H$ to denote the HIV status ($1$ if positive and $0$ if negative). :numref:conditional_prob_D1 lists such conditional probabilities. . :Conditional probability of $P(D_1 mid H)$. . Conditional probability $H=1$ $H=0$ . $P(D_1 = 1 mid H)$ | 1 | 0.01 | . $P(D_1 = 0 mid H)$ | 0 | 0.99 | . :label:conditional_prob_D1 . Note that the column sums are all 1 (but the row sums are not), since the conditional probability needs to sum up to 1, just like the probability. Let us work out the probability of the patient having AIDS if the test comes back positive, i.e., $P(H = 1 mid D_1 = 1)$. Obviously this is going to depend on how common the disease is, since it affects the number of false alarms. Assume that the population is quite healthy, e.g., $P(H=1) = 0.0015$. To apply Bayes&#39; theorem, we need to apply marginalization and the multiplication rule to determine . $$ begin{aligned} &amp;P(D_1 = 1) =&amp; P(D_1=1, H=0) + P(D_1=1, H=1) =&amp; P(D_1=1 mid H=0) P(H=0) + P(D_1=1 mid H=1) P(H=1) =&amp; 0.011485. end{aligned} $$Thus, we get . $$ begin{aligned} &amp;P(H = 1 mid D_1 = 1) =&amp; frac{P(D_1=1 mid H=1) P(H=1)}{P(D_1=1)} =&amp; 0.1306 end{aligned}.$$In other words, there is only a 13.06% chance that the patient actually has AIDS, despite using a very accurate test. As we can see, probability can be counterintuitive. . What should a patient do upon receiving such terrifying news? Likely, the patient would ask the physician to administer another test to get clarity. The second test has different characteristics and it is not as good as the first one, as shown in :numref:conditional_prob_D2. . :Conditional probability of $P(D_2 mid H)$. . Conditional probability $H=1$ $H=0$ . $P(D_2 = 1 mid H)$ | 0.98 | 0.03 | . $P(D_2 = 0 mid H)$ | 0.02 | 0.97 | . :label:conditional_prob_D2 . Unfortunately, the second test comes back positive, too. Let us work out the requisite probabilities to invoke Bayes&#39; theorem by assuming the conditional independence: . $$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1 mid H = 0) =&amp; P(D_1 = 1 mid H = 0) P(D_2 = 1 mid H = 0) =&amp; 0.0003, end{aligned} $$$$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1 mid H = 1) =&amp; P(D_1 = 1 mid H = 1) P(D_2 = 1 mid H = 1) =&amp; 0.98. end{aligned} $$Now we can apply marginalization and the multiplication rule: . $$ begin{aligned} &amp;P(D_1 = 1, D_2 = 1) =&amp; P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1) =&amp; P(D_1 = 1, D_2 = 1 mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 mid H = 1)P(H=1) =&amp; 0.00176955. end{aligned} $$In the end, the probability of the patient having AIDS given both positive tests is . $$ begin{aligned} &amp;P(H = 1 mid D_1 = 1, D_2 = 1) =&amp; frac{P(D_1 = 1, D_2 = 1 mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} =&amp; 0.8307. end{aligned} $$That is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one, it still significantly improved our estimate. . Expectation and Variance . To summarize key characteristics of probability distributions, we need some measures. The expectation (or average) of the random variable $X$ is denoted as . $$E[X] = sum_{x} x P(X = x).$$ . When the input of a function $f(x)$ is a random variable drawn from the distribution $P$ with different values $x$, the expectation of $f(x)$ is computed as . $$E_{x sim P}[f(x)] = sum_x f(x) P(x).$$ . In many cases we want to measure by how much the random variable $X$ deviates from its expectation. This can be quantified by the variance . $$ mathrm{Var}[X] = E left[(X - E[X])^2 right] = E[X^2] - E[X]^2.$$Its square root is called the standard deviation. The variance of a function of a random variable measures by how much the function deviates from the expectation of the function, as different values $x$ of the random variable are sampled from its distribution: . $$ mathrm{Var}[f(x)] = E left[ left(f(x) - E[f(x)] right)^2 right].$$ . Summary . We can sample from probability distributions. | We can analyze multiple random variables using joint distribution, conditional distribution, Bayes&#39; theorem, marginalization, and independence assumptions. | Expectation and variance offer useful measures to summarize key characteristics of probability distributions. | . Exercises . We conducted $m=500$ groups of experiments where each group draws $n=10$ samples. Vary $m$ and $n$. Observe and analyze the experimental results. | Given two events with probability $P( mathcal{A})$ and $P( mathcal{B})$, compute upper and lower bounds on $P( mathcal{A} cup mathcal{B})$ and $P( mathcal{A} cap mathcal{B})$. (Hint: display the situation using a Venn Diagram.) | Assume that we have a sequence of random variables, say $A$, $B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? (Hint: this is a Markov Chain.) | In :numref:subsec_probability_hiv_app, the first test is more accurate. Why not just run the first test a second time? | Discussions .",
            "url": "https://unverciftci.github.io/derin_ogrenme/2020/02/22/olasilik.html",
            "relUrl": "/2020/02/22/olasilik.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This is a $f(x)$ demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://unverciftci.github.io/derin_ogrenme/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Bilgi",
          "content": "Bu kitap Derin Öğrenme konusunda bazı temel konuları içermektedir. . Hata ve eklemeler için: unverciftci@gmail.com. .",
          "url": "https://unverciftci.github.io/derin_ogrenme/bilgi/",
          "relUrl": "/bilgi/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://unverciftci.github.io/derin_ogrenme/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}